[**ä¸­æ–‡è¯´æ˜**](README.md) | [**English**](README_En.md)

<p align="center">
    <br>
    <img src="assets/Chinese_CLIP_logo_tp_path.svg" width="400" />
    <br>
<p>
<br>

<p align="center">
        <a href="https://www.modelscope.cn/models?name=clip&tasks=multi-modal-embedding">ModelScope</a>&nbsp ï½œ &nbsp<a href="https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://arxiv.org/abs/2211.01335">Paper</a>&nbsp ï½œ &nbspBlog
</p>
<br><br>

æœ¬é¡¹ç›®ä¸ºCLIPæ¨¡å‹çš„**ä¸­æ–‡**ç‰ˆæœ¬ï¼Œä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆ~2äº¿å›¾æ–‡å¯¹ï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®ç°ä¸­æ–‡é¢†åŸŸçš„[å›¾æ–‡ç‰¹å¾&ç›¸ä¼¼åº¦è®¡ç®—](#APIå¿«é€Ÿä¸Šæ‰‹)ã€[è·¨æ¨¡æ€æ£€ç´¢](#è·¨æ¨¡æ€æ£€ç´¢)ã€[é›¶æ ·æœ¬å›¾ç‰‡åˆ†ç±»](#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ç­‰ä»»åŠ¡ã€‚æœ¬é¡¹ç›®ä»£ç åŸºäº<b>[open_clip project](https://github.com/mlfoundations/open_clip)</b>å»ºè®¾ï¼Œå¹¶é’ˆå¯¹ä¸­æ–‡é¢†åŸŸæ•°æ®ä»¥åŠåœ¨ä¸­æ–‡æ•°æ®ä¸Šå®ç°æ›´å¥½çš„æ•ˆæœåšäº†ä¼˜åŒ–ã€‚æœ¬é¡¹ç›®æä¾›äº†APIã€è®­ç»ƒä»£ç å’Œæµ‹è¯•ä»£ç ï¼Œä¸‹æ–‡ä¸­å°†è¯¦ç»†ä»‹ç»ç»†èŠ‚ã€‚
<br><br>

# æ–°é—»
* 2022.12.3 å…¬å¼€[ELEVATER](https://eval.ai/web/challenges/challenge-page/1832)å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸­æ–‡ç‰ˆæœ¬ï¼Œè¯¦è§[æ•°æ®æ–‡æ¡£](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/zeroshot_dataset.md)
* 2022.12.1 Chinese-CLIPæ¨¡å‹ä»£ç &ç‰¹å¾æå–APIï¼ŒåŒæ­¥åˆå…¥Huggingface transformersğŸ¤—ä»£ç åº“
* 2022.11.22 æ–°å¢[é›¶æ ·æœ¬å›¾åƒåˆ†ç±»](#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ä»£ç ï¼Œå¯æ”¯æŒ[ELEVATER benchmark](https://eval.ai/web/challenges/challenge-page/1832)é›¶æ ·æœ¬åˆ†ç±»è¯„æµ‹ä»»åŠ¡
* 2022.11.3 æ–°å¢RN50ï¼ŒViT-H-14æ¨¡å‹ï¼Œå…¬å¼€[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2211.01335.pdf)
* 2022.9.22 æ–°å¢ViT-L-14ï¼ŒViT-L-14-336æ¨¡å‹
* 2022.7.13 æ–°å¢[å›¾æ–‡ç‰¹å¾æå–å¿«é€ŸAPI](#APIå¿«é€Ÿä¸Šæ‰‹)ï¼Œå‡ è¡Œä»£ç å¿«é€Ÿè°ƒç”¨ä¸­æ–‡CLIPæ¨¡å‹ï¼Œè®¡ç®—å›¾æ–‡ç‰¹å¾&ç›¸ä¼¼åº¦
* 2022.7.8 Chinese-CLIPé¡¹ç›®æ­£å¼å¼€æºï¼Œå¼€æº[å›¾æ–‡æ£€ç´¢](#è·¨æ¨¡æ€æ£€ç´¢)ä»£ç 
<br><br>

# æ¨¡å‹åŠå®éªŒ
<span id="model_card"></span>
## æ¨¡å‹è§„æ¨¡ & ä¸‹è½½é“¾æ¥
Chinese-CLIPç›®å‰å¼€æº5ä¸ªä¸åŒè§„æ¨¡ï¼Œå…¶æ¨¡å‹ä¿¡æ¯å’Œä¸‹è½½æ–¹å¼è§ä¸‹è¡¨ï¼š

<table border="1" width="100%">
    <tr align="center">
        <th>æ¨¡å‹è§„æ¨¡</th><th>ä¸‹è½½é“¾æ¥</th><th>å‚æ•°é‡</th><th>è§†è§‰ä¾§éª¨æ¶</th><th>è§†è§‰ä¾§å‚æ•°é‡</th><th>æ–‡æœ¬ä¾§éª¨æ¶</th><th>æ–‡æœ¬ä¾§å‚æ•°é‡</th><th>åˆ†è¾¨ç‡</th>
    </tr>
    <tr align="center">
        <td>CN-CLIP<sub>RN50</sub></td><td><a href="https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_rn50.pt">Download</a></td><td>77M</td><td>ResNet50</td><td>38M</td><td>RBT3</td><td>39M</td><td>224</td>
    </tr>
    <tr align="center">
        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href="https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-b-16.pt">Download</a></td><td>188M</td><td>ViT-B/16</td><td>86M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>
    </tr>
    <tr align="center">
        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href="https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14.pt">Download</a></td><td>406M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>224</td>
    </tr>
    <tr align="center">
        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href="https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-l-14-336.pt">Download</a></td><td>407M</td><td>ViT-L/14</td><td>304M</td><td>RoBERTa-wwm-Base</td><td>102M</td><td>336</td>
    </tr>
    <tr align="center">
        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href="https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-h-14.pt">Download</a></td><td>958M</td><td>ViT-H/14</td><td>632M</td><td>RoBERTa-wwm-Large</td><td>326M</td><td>224</td>
    </tr>
</table>
<br></br>

## å®éªŒç»“æœ
é’ˆå¯¹å›¾æ–‡æ£€ç´¢ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨[MUGE Retrieval](https://tianchi.aliyun.com/muge)ã€[Flickr30K-CN](https://github.com/li-xirong/cross-lingual-cap)å’Œ[COCO-CN](https://github.com/li-xirong/coco-cn)ä¸Šè¿›è¡Œäº†zero-shotå’Œfinetuneçš„å®éªŒã€‚é’ˆå¯¹å›¾åƒé›¶æ ·æœ¬åˆ†ç±»ï¼Œæˆ‘ä»¬åœ¨[ELEVATER](https://eval.ai/web/challenges/challenge-page/1832)çš„10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚ç¯‡å¹…æ‰€é™ï¼Œæˆ‘ä»¬è¿™é‡Œç»™å‡ºbaselineæ¨¡å‹å’ŒChinese-CLIPçš„æœ€å¤§è§„æ¨¡æ¨¡å‹ç»“æœï¼Œå…³äºChinese-CLIPå„è§„æ¨¡çš„è¯¦ç»†ç»“æœæŒ‡æ ‡ï¼Œè¯·è¯¦è§[Results.md](Results.md)ã€‚

**MUGE Text-to-Image Retrieval**:
<table border="1" width="100%">
    <tr align="center">
        <th>Setup</th><th colspan="4">Zero-shot</th><th colspan="4">Finetune</th>
    </tr>
    <tr align="center">
        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>
    </tr>
	<tr align="center">
        <td width="120%">Wukong</td><td>42.7</td><td>69.0</td><td>78.0</td><td>63.2</td><td>52.7</td><td>77.9</td><td>85.6</td><td>72.1</td>
    </tr>
	<tr align="center">
        <td width="120%">R2D2</td><td>49.5</td><td>75.7</td><td>83.2</td><td>69.5</td><td>60.1</td><td>82.9</td><td>89.4</td><td>77.5</td>
    </tr>
	<tr align="center">
        <td width="120%">CN-CLIP</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td><td>68.9</td><td>88.7</td><td>93.1</td><td>83.6</td>
    </tr>
</table>
<br>

**Flickr30K-CN Retrieval**:
<table border="1" width="150%">
	<tr align="center">
        <th>Task</th><th colspan="6">Text-to-Image</th><th colspan="6">Image-to-Text</th>
    </tr>
    <tr align="center">
        <th>Setup</th><th colspan="3">Zero-shot</th><th colspan="3">Finetune</th><th colspan="3">Zero-shot</th><th colspan="3">Finetune</th>
    </tr>
    <tr align="center">
        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>
    </tr>
	<tr align="center">
        <td width="120%">Wukong</td><td>51.7</td><td>78.9</td><td>86.3</td><td>77.4</td><td>94.5</td><td>97.0</td><td>76.1</td><td>94.8</td><td>97.5</td><td>92.7</td><td>99.1</td><td>99.6</td>
    </tr>
	<tr align="center">
        <td width="120%">R2D2</td><td>60.9</td><td>86.8</td><td>92.7</td><td>84.4</td><td>96.7</td><td>98.4</td><td>77.6</td><td>96.7</td><td>98.9</td><td>95.6</td><td>99.8</td><td>100.0</td>
    </tr>
	<tr align="center">
        <td width="120%">CN-CLIP</td><td>71.2</td><td>91.4</td><td>95.5</td><td>83.8</td><td>96.9</td><td>98.6</td><td>81.6</td><td>97.5</td><td>98.8</td><td>95.3</td><td>99.7</td><td>100.0</td>
    </tr>
</table>
<br>

**COCO-CN Retrieval**:
<table border="1" width="150%">
	<tr align="center">
        <th>Task</th><th colspan="6">Text-to-Image</th><th colspan="6">Image-to-Text</th>
    </tr>
    <tr align="center">
        <th>Setup</th><th colspan="3">Zero-shot</th><th colspan="3">Finetune</th><th colspan="3">Zero-shot</th><th colspan="3">Finetune</th>
    </tr>
    <tr align="center">
        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td>
    </tr>
	<tr align="center">
        <td width="150%">Wukong</td><td>53.4</td><td>80.2</td><td>90.1</td><td>74.0</td><td>94.4</td><td>98.1</td><td>55.2</td><td>81.0</td><td>90.6</td><td>73.3</td><td>94.0</td><td>98.0</td>
    </tr>
	<tr align="center">
        <td width="150%">R2D2</td><td>56.4</td><td>85.0</td><td>93.1</td><td>79.1</td><td>96.5</td><td>98.9</td><td>63.3</td><td>89.3</td><td>95.7</td><td>79.3</td><td>97.1</td><td>98.7</td>
    </tr>
	<tr align="center">
        <td width="150%">CN-CLIP</td><td>69.2</td><td>89.9</td><td>96.1</td><td>81.5</td><td>96.9</td><td>99.1</td><td>63.0</td><td>86.6</td><td>92.9</td><td>83.5</td><td>97.3</td><td>99.2</td>
    </tr>
</table>
<br>

**Zero-shot Image Classification**:
<table border="1" width="150%">
	<tr align="center">
        <th>Task</th><th>CIFAR10</th><th>CIFAR100</th><th>DTD</th><th>EuroSAT</th><th>FER</th><th>FGVC</th><th>KITTI</th><th>MNIST</th><th>PC</th><th>VOC</th>
    </tr>
	<tr align="center">
        <td width="150%">GIT</td><td>88.5</td><td>61.1</td><td>42.9</td><td>43.4</td><td>41.4</td><td>6.7</td><td>22.1</td><td>68.9</td><td>50.0</td><td>80.2</td>
    </tr>
    	<tr align="center">
        <td width="150%">ALIGN</td><td>94.9</td><td>76.8</td><td>66.1</td><td>52.1</td><td>50.8</td><td>25.0</td><td>41.2</td><td>74.0</td><td>55.2</td><td>83.0</td>
    </tr>
	<tr align="center">
        <td width="150%">CLIP</td><td>94.9</td><td>77.0</td><td>56.0</td><td>63.0</td><td>48.3</td><td>33.3</td><td>11.5</td><td>79.0</td><td>62.3</td><td>84.0</td>
    </tr>
    	<tr align="center">
        <td width="150%">Wukong</td><td>95.4</td><td>77.1</td><td>40.9</td><td>50.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td>
    </tr>
    	<tr align="center">
        <td width="150%">CN-CLIP</td><td>96.0</td><td>79.7</td><td>51.2</td><td>52.0</td><td>55.1</td><td>26.2</td><td>49.9</td><td>79.4</td><td>63.5</td><td>84.9</td>
    </tr>
</table>

<br><br>


# å¼€å§‹ç”¨èµ·æ¥ï¼
## å®‰è£…è¦æ±‚
å¼€å§‹æœ¬é¡¹ç›®å‰ï¼Œéœ€å…ˆæ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¸‹åˆ—ç¯å¢ƒé…ç½®è¦æ±‚:

* python >= 3.6.4
* pytorch >= 1.8.0 (with torchvision >= 0.9.0)
* CUDA Version >= 10.2

è¿è¡Œä¸‹åˆ—å‘½ä»¤å³å¯å®‰è£…æœ¬é¡¹ç›®æ‰€éœ€çš„ä¸‰æ–¹åº“ã€‚

```bash
pip install -r requirements.txt
```

## APIå¿«é€Ÿä¸Šæ‰‹
ä¸‹é¢æä¾›ä¸€æ®µç®€å•çš„ä»£ç ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨ä¸­æ–‡CLIPçš„APIã€‚å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆå®‰è£…cn_clipï¼š
```bash
# é€šè¿‡pipå®‰è£…
pip install cn_clip

# æˆ–è€…ä»æºä»£ç å®‰è£…
cd Chinese-CLIP
pip install -e .
```
å®‰è£…æˆåŠŸåï¼Œå³å¯é€šè¿‡å¦‚ä¸‹æ–¹å¼è½»æ¾è°ƒç”¨APIï¼Œæå–å›¾æ–‡ç‰¹å¾å¹¶è®¡ç®—ç›¸ä¼¼åº¦ï¼š
```python
import torch 
from PIL import Image

import cn_clip.clip as clip
from cn_clip.clip import load_from_name, available_models
print("Available models:", available_models())  
# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = load_from_name("ViT-B-16", device=device, download_root='./')
model.eval()
image = preprocess(Image.open("examples/pokemon.jpeg")).unsqueeze(0).to(device)
text = clip.tokenize(["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"]).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    # å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¯·ä½¿ç”¨å½’ä¸€åŒ–åçš„å›¾æ–‡ç‰¹å¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡
    image_features /= image_features.norm(dim=-1, keepdim=True) 
    text_features /= text_features.norm(dim=-1, keepdim=True)    

    logits_per_image, logits_per_text = model.get_similarity(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print("Label probs:", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]
```
å¦‚æœä½ ä¸æ»¡è¶³äºä»…ä»…ä½¿ç”¨APIï¼Œæ¬¢è¿ç»§ç»­é˜…è¯»æœ¬æ–‡æ¡£ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„é¡¹ç›®è¿›è¡ŒCLIPæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚
<br><br>


# æ•™ç¨‹
ä¸‹æ–‡å°†åŒ…æ‹¬[è·¨æ¨¡æ€æ£€ç´¢æ•™ç¨‹](#è·¨æ¨¡æ€æ£€ç´¢)ï¼ˆåŒ…å«finetuneå’Œinferenceï¼ŒåŠKNNè®¡ç®—ç­‰ï¼‰ä»¥åŠ[é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ•™ç¨‹](#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ã€‚

## è·¨æ¨¡æ€æ£€ç´¢
### ä»£ç ç»„ç»‡
ä¸‹è½½æœ¬é¡¹ç›®å, è¯·åˆ›å»ºæ–°çš„æ–‡ä»¶å¤¹ ```${DATAPATH}``` ä»¥å­˜æ”¾æ•°æ®é›†ã€é¢„è®­ç»ƒckptã€ä»¥åŠfinetuneäº§ç”Ÿçš„æ¨¡å‹æ—¥å¿—&ckptã€‚æ¨èå·¥ä½œåŒºç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
Chinese-CLIP/
â”œâ”€â”€ run_scripts/
â”‚   â”œâ”€â”€ muge_finetune_vit-b-16_rbt-base.sh
â”‚   â”œâ”€â”€ flickr30k_finetune_vit-b-16_rbt-base.sh
â”‚   â””â”€â”€ ...           # æ›´å¤šfinetuneæˆ–è¯„æµ‹è„šæœ¬...
â””â”€â”€ cn_clip/
    â”œâ”€â”€ clip/
    â”œâ”€â”€ eval/
    â”œâ”€â”€ preprocess/
    â””â”€â”€ training/

${DATAPATH}
â”œâ”€â”€ pretrained_weights/
â”œâ”€â”€ experiments/
â””â”€â”€ datasets/
    â”œâ”€â”€ MUGE/
    â”œâ”€â”€ Flickr30k-CN/
    â””â”€â”€ .../          # æ›´å¤šè‡ªå®šä¹‰æ•°æ®é›†...
```

### å‡†å¤‡å·¥ä½œ
è¿™é‡Œæˆ‘ä»¬æä¾›é¢„è®­ç»ƒæ¨¡å‹å‚æ•°çš„ä¸‹è½½æ–¹å¼ï¼Œä»¥åŠè¿›è¡Œfinetuneå‰å¯¹æ•°æ®è¿›è¡Œçš„é¢„å¤„ç†è¿‡ç¨‹

#### é¢„è®­ç»ƒCKPT

è¯·å‚è€ƒå‰æ–‡[æ¨¡å‹è§„æ¨¡ & ä¸‹è½½é“¾æ¥](#model_card)éƒ¨åˆ†ï¼Œä¸‹è½½å¯¹åº”æ¨¡å‹ckptã€‚æ¨èå°†ä¸‹è½½çš„ckptæ–‡ä»¶å­˜æ”¾äº`${DATAPATH}/pretrained_weights/`ç›®å½•ä¸‹ã€‚

#### æ•°æ®é›†æ ¼å¼é¢„å¤„ç†

ä¸ºäº†ä¸Chinese-CLIPä»£ç é€‚é…ï¼ŒåŒæ—¶ä¿è¯æ•°æ®å¤„ç†å’Œè¯»å–çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®å°†è®­ç»ƒ&è¯„æµ‹ä½¿ç”¨çš„å›¾æ–‡æ•°æ®é›†ç»Ÿä¸€ç»„ç»‡æˆå¦‚ä¸‹çš„æ–¹å¼ï¼š

```
${DATAPATH}
â””â”€â”€ datasets/
    â””â”€â”€ ${dataset_name}/
        â”œâ”€â”€ train_imgs.tsv      # å›¾ç‰‡id & å›¾ç‰‡å†…å®¹
        â”œâ”€â”€ train_texts.jsonl   # æ–‡æœ¬id & æ–‡æœ¬å†…å®¹ï¼Œè¿åŒåŒ¹é…çš„å›¾ç‰‡idåˆ—è¡¨
        â”œâ”€â”€ valid_imgs.tsv
        â”œâ”€â”€ valid_texts.jsonl
        â”œâ”€â”€ test_imgs.tsv
        â””â”€â”€ test_texts.jsonl
```
å…¶ä¸­`${dataset_name}`ä»£æŒ‡æ•°æ®é›†åç§°ï¼ˆå¦‚MUGEï¼‰

ä¸ºä¿è¯æ–‡ä»¶å¤„ç†æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸æ˜¯å°†å›¾ç‰‡ä»¥å¤§é‡çš„å°æ–‡ä»¶æ–¹å¼å­˜æ”¾ï¼Œè€Œæ˜¯å°†è®­ç»ƒ/éªŒè¯/æµ‹è¯•å›¾ç‰‡ä»¥base64å½¢å¼åˆ†åˆ«å­˜æ”¾åœ¨`${split}_imgs.tsv`æ–‡ä»¶ä¸­ã€‚æ–‡ä»¶æ¯è¡Œè¡¨ç¤ºä¸€å¼ å›¾ç‰‡ï¼ŒåŒ…å«å›¾ç‰‡idï¼ˆintå‹ï¼‰ä¸å›¾ç‰‡base64ï¼Œä»¥tabéš”å¼€ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```
1000002	/9j/4AAQSkZJ...YQj7314oA//2Q==
```

å°†å›¾ç‰‡åŸå§‹æ–‡ä»¶è½¬æ¢ä¸ºbase64çš„æ–¹å¼éå¸¸ç®€å•ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹pythonä»£ç ï¼š
```python
from PIL import Image
from io import BytesIO
import base64

img = Image.open(file_name) # è®¿é—®å›¾ç‰‡è·¯å¾„
img_buffer = BytesIO()
img.save(img_buffer, format=img.format)
byte_data = img_buffer.getvalue()
base64_str = base64.b64encode(byte_data) # bytes
base64_str = base64_str.decode("utf-8") # str
```

æ–‡æœ¬ä¿¡æ¯åŠå›¾æ–‡å¯¹åŒ¹é…å…³ç³»åˆ™ä¿å­˜åœ¨`${split}_texts.jsonl`æ–‡ä»¶ã€‚æ–‡ä»¶æ¯è¡Œæ˜¯ä¸€è¡Œjsonï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```
{"text_id": 8428, "text": "é«˜çº§æ„Ÿæ‰˜ç‰¹åŒ…æ–œæŒ", "image_ids": [1076345, 517602]}
```
å¯¹äºæµ‹è¯•é›†åªæœ‰æ–‡æœ¬ï¼Œä¸çŸ¥é“å›¾æ–‡å¯¹åŒ¹é…å…³ç³»çš„æƒ…å†µï¼Œæ¯è¡Œçš„`image_ids`å­—æ®µå¤„ç†ä¸ºç©ºåˆ—è¡¨å³å¯ï¼Œå³`"image_ids": []`ã€‚

æœ€åï¼Œæˆ‘ä»¬è¿˜éœ€è¦å°†tsvå’Œjsonlæ–‡ä»¶ä¸€èµ·åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå†…å­˜ç´¢å¼•çš„LMDBæ•°æ®åº“æ–‡ä»¶ï¼Œæ–¹ä¾¿è®­ç»ƒæ—¶çš„éšæœºè¯»å–
```
python cn_clip/preprocess/build_lmdb_dataset.py \
    --data_dir ${DATAPATH}/datasets/${dataset_name}
    --splits train,valid,test
```
ä¾‹å¦‚å¯¹äºMUGEæ•°æ®é›†ï¼Œåˆ™`${dataset_name}`è®¾ä¸ºMUGEï¼Œ`--splits`æŒ‡å®šéœ€è¦è½¬æ¢çš„æ•°æ®é›†åˆ’åˆ†ï¼Œä»¥é€—å·ä¸åŠ ç©ºæ ¼åˆ†éš”ã€‚è½¬æ¢åï¼Œæ•°æ®é›†æ–‡ä»¶å¤¹ä¸‹ä¼šå¯¹åº”å¢åŠ ä»¥ä¸‹LMDBåºåˆ—åŒ–æ–‡ä»¶
```
${DATAPATH}
â””â”€â”€ datasets/
    â””â”€â”€ ${dataset_name}/
        â””â”€â”€ lmdb/
            â”œâ”€â”€ train
            â”‚Â Â  â”œâ”€â”€ imgs
            â”‚Â Â  â””â”€â”€ pairs
            â”œâ”€â”€ valid
            â””â”€â”€ test
```

ä¸ºäº†é™ä½ä¸Šæ‰‹éš¾åº¦ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›äº†æŒ‰ä¸Šè¿°æ­¥éª¤é¢„å¤„ç†å¥½çš„MUGEæ•°æ®ï¼ˆ[ä¸‹è½½é“¾æ¥](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/MUGE.zip)ï¼‰å’ŒFlickr30K-CNæ•°æ®ï¼ˆ[ä¸‹è½½é“¾æ¥](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/datasets/Flickr30k-CN.zip)ï¼‰å‹ç¼©åŒ…ï¼Œç›´æ¥ä¸‹è½½è§£å‹å¹¶æ”¾ç½®äº`${DATAPATH}/datasets/`ç›®å½•ä¸‹å³å¯ã€‚

### æ¨¡å‹finetune

åœ¨æ­¤æˆ‘ä»¬ä»‹ç»è®­ç»ƒçš„æ­¥éª¤ï¼Œæ–¹ä¾¿å…¶ä»–ç”¨æˆ·äº†è§£æ¨¡å‹ç»†èŠ‚ï¼Œä½¿ç”¨æˆ‘ä»¬æä¾›çš„ä¸­æ–‡CLIPé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œfinetuneã€‚åŸºäºMUGEå’ŒFlickr30K-CNä¸¤ä¸ªä¸‹æ¸¸æ£€ç´¢æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›äº†è®­ç»ƒæ ·ä¾‹è„šæœ¬`run_scripts/muge_finetune_vit-b-16_rbt-base.sh`å’Œ`run_scripts/flickr30k_finetune_vit-b-16_rbt-base.sh`ã€‚<b>è¿è¡Œè„šæœ¬åŒæ—¶æ”¯æŒå•æœºå’Œå¤šæœºåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·åœ¨è¿è¡Œå‰ï¼Œå…ˆæ ¹æ®è„šæœ¬å¼€å¤´çš„æŒ‡å¼•æ³¨é‡Šï¼Œå¡«å†™å¥½åˆ†å¸ƒå¼ç›¸å…³é…ç½®ï¼Œä¹‹åè¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯å¼€å§‹è®­ç»ƒï¼ˆå¤šæœºè®­ç»ƒè¯·åœ¨å„æœºå™¨ä¸Šéƒ½è¿è¡Œå‘½ä»¤ï¼‰ã€‚å¯¹äºæ˜¾å­˜ä¸è¶³çš„æƒ…å†µï¼Œå¯ä»¥è€ƒè™‘æ¿€æ´»é…ç½®é¡¹ä¸­çš„[é‡è®¡ç®—ç­–ç•¥](#checkpointing)ã€‚</b>è®­ç»ƒäº§ç”Ÿçš„logå’Œæ¨¡å‹ckptæ–‡ä»¶ï¼Œä¼šè‡ªåŠ¨ä¿å­˜åœ¨ç”¨æˆ·æŒ‡å®šçš„ç›®å½•ä¸‹ï¼š

```bash
cd Chinese-CLIP/
bash run_scripts/muge_finetune_vit-b-16_rbt-base.sh ${DATAPATH}
```

ç›¸å…³çš„è®­ç»ƒé…ç½®é¡¹åŒ…æ‹¬:

+ åˆ†å¸ƒå¼
  + `WORKER_CNT`: è®­ç»ƒçš„æœºå™¨ä¸ªæ•°
  + `GPUS_PER_NODE`: æ¯ä¸ªæœºå™¨ä¸Šçš„GPUä¸ªæ•°
+ è®­ç»ƒ/éªŒè¯æ•°æ®
  + `train-data`: è®­ç»ƒæ•°æ®LMDBç›®å½•ï¼Œå‡†å¤‡LMDBæ•°æ®æ–‡ä»¶çš„é¢„å¤„ç†æµç¨‹è§ä¸Šã€‚
  + `val-data`: éªŒè¯æ•°æ®LMDBç›®å½•ã€‚
  + `num-workers`: è®­ç»ƒæ•°æ®å¤„ç†ï¼ˆDataLoaderï¼‰çš„è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4
+ è®­ç»ƒè¶…å‚æ•°
  + `vision-model`: æŒ‡å®šè§†è§‰backbone, ä» `["ViT-B-16", "ViT-L-14", "ViT-L-14-336", "ViT-H-14", "RN50"]`é€‰æ‹©ã€‚
  + `text-model`: æŒ‡å®šæ–‡æœ¬backbone, ä» `["RoBERTa-wwm-ext-base-chinese", "RoBERTa-wwm-ext-large-chinese", "RBT3-chinese"]`é€‰æ‹©ã€‚
  + `context-length`: æ–‡æœ¬è¾“å…¥åºåˆ—é•¿åº¦ã€‚
  + `warmup`: warmupæ­¥æ•°ã€‚
  + `batch-size`: è®­ç»ƒæ—¶å•å¡batch-sizeã€‚ï¼ˆè¯·ä¿è¯`è®­ç»ƒæ ·æœ¬æ€»æ•° > batch-size * GPUæ•°`ï¼Œè‡³å°‘æ»¡è¶³1ä¸ªè®­ç»ƒbatchï¼‰
  + `lr`: å­¦ä¹ ç‡ã€‚
  + `wd`: weight decayã€‚
  + `max-steps`: è®­ç»ƒæ­¥æ•°ï¼Œä¹Ÿå¯é€šè¿‡`max-epochs`æŒ‡å®šè®­ç»ƒè½®æ•°ã€‚
  + `freeze-vision`: æ˜¯å¦freezeè§†è§‰backboneã€‚
  + `use-augment`: æ˜¯å¦ä½¿ç”¨[AutoAugment](https://arxiv.org/abs/1805.09501)å¯¹å›¾ç‰‡è¿›è¡Œæ•°æ®å¢å¼ºã€‚
  + `valid-batch-size`: éªŒè¯æ—¶å•æœºbatch-sizeã€‚ï¼ˆè¯·ä¿è¯`éªŒè¯é›†æ ·æœ¬æ€»æ•° > batch-size * GPUæ•°`ï¼Œè‡³å°‘æ»¡è¶³1ä¸ªéªŒè¯batchï¼‰
  + `valid-step-interval`å’Œ`valid-epoch-interval`: éªŒè¯step/epoché¢‘ç‡ï¼ŒæŒ‡å®šä¸º-1æ—¶åˆ™åœ¨è®­ç»ƒä¸­ä¸è¿›è¡ŒéªŒè¯ã€‚
  + `grad-checkpointing`: <span id="checkpointing"></span>ä½¿ç”¨[é‡è®¡ç®—ç­–ç•¥](https://pytorch.org/docs/stable/checkpoint.html)ï¼Œåœ¨å‰å‘è¿‡ç¨‹ä¸­ä¸ä¿å­˜ä¸­é—´ç»“æœï¼Œä»¥è®­ç»ƒæ—¶é—´æ¢å–æ›´å°çš„æ˜¾å­˜å¼€é”€ï¼Œé€‚ç”¨äºæ˜¾å­˜ä¸è¶³çš„æƒ…å†µã€‚ï¼ˆ`store_true`å‚æ•°ï¼Œç›´æ¥åœ¨è„šæœ¬ä¸­åŠ ä¸Š`--grad-checkpointing`å³å¯ï¼Œç›®å‰è¦æ±‚Pytorch>1.8.0ï¼‰
+ è¾“å‡ºé€‰é¡¹
  + `name`: æŒ‡å®šè¾“å‡ºè·¯å¾„ã€‚è¶…å‚æ—¥å¿—, è®­ç»ƒæ—¥å¿—ä»¥åŠäº§å‡ºckptå‡ä¼šå­˜æ”¾è‡³ `${DATAPATH}/experiments/${name}/`ã€‚
  + `save-step-frequency`åŠ`save-epoch-frequency`: å­˜ckptçš„æ­¥æ•°æˆ–è½®æ•°é—´éš”ã€‚
  + `report-training-batch-acc`: æ—¥å¿—æ˜¯å¦æŠ¥å‘Šè®­ç»ƒå›¾åˆ°æ–‡&æ–‡åˆ°å›¾batchå‡†ç¡®ç‡ã€‚
+ æƒé‡è¯»å–ç›¸å…³é€‰é¡¹
  + `resume`: æƒé‡è¯»å–çš„è·¯å¾„ã€‚ç¤ºä¾‹è„šæœ¬ä¸­æŒ‡å®šä¸ºé¢„è®­ç»ƒckptè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸ºç”¨æˆ·è‡ªå·±finetuneçš„ckptè·¯å¾„åšç»§ç»­è®­ç»ƒã€‚
  + `reset-data-offset`: æ˜¯å¦ä»æ­¤å‰çš„æ•°æ®æ–­ç‚¹ç»­è·‘ã€‚å¦‚batch sizeæˆ–GPUå¡æ•°è¶…å‚æ”¹å˜ï¼Œå»ºè®®æ‰“å¼€æ­¤é€‰é¡¹ã€‚
  + `reset-optimizer`: æ˜¯å¦ä½¿ç”¨optimizer stateã€‚

è®­ç»ƒå®Œæ¯•ï¼Œlog ä¼šè‡ªåŠ¨å­˜åœ¨`${DATAPATH}/experiments/${name}/out_${timestamp}.log`ï¼Œè®­ç»ƒlogæ ¼å¼å¦‚ä¸‹æ‰€ç¤º:
```
2022-06-16,10:58:27 | INFO | Rank 0 | Global Steps: 1/735 | Train Epoch: 1 [1024/250880 (0%)] | Loss: 2.171807 | Image2Text Acc: 49.41 | Text2Image Acc: 52.54 | Data Time: 5.167s | Batch Time: 15.647s | LR: 0.000000 | logit_scale: 4.605 | Global Batch Size: 1024
```
éªŒè¯logæ ¼å¼å¦‚ä¸‹æ‰€ç¤º:
```
2022-06-16,11:06:00 | INFO | Rank 0 | Validation Result (epoch 1 @ 150 steps) | Valid Loss: 0.503617 | Image2Text Acc: 84.76 | Text2Image Acc: 84.37 | logit_scale: 4.605 | Valid Batch Size: 128
```

**æ³¨æ„**: å¯¹æ¯”å­¦ä¹ çš„è®­ç»ƒæ”¶æ•›å’Œç¨³å®šæ€§å’Œæ€»batch sizeç›¸å…³ã€‚å¦‚æ‚¨ä½¿ç”¨æ›´å°çš„batch sizeï¼ˆç›¸æ¯”é»˜è®¤é…ç½®128 per-GPU \* 8 GPUï¼‰ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬æ¨èä½¿ç”¨æ›´å¤šçš„GPUå’Œæ›´å¤§çš„batch sizeä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚

### é¢„æµ‹åŠè¯„ä¼°

æˆ‘ä»¬æä¾›ç‰¹å¾æå–ã€ä»¥åŠå›¾æ–‡æ£€ç´¢ä»»åŠ¡è¯„ä¼°çš„æµç¨‹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

#### å›¾æ–‡ç‰¹å¾æå–

ç›®å‰æœ¬ä»£ç æ”¯æŒä½¿ç”¨GPUå•å¡è¿›è¡Œå›¾æ–‡ç‰¹å¾æå–ï¼Œè¯·å‚è€ƒä½¿ç”¨ä»¥ä¸‹å‘½ä»¤
```bash
cd Chinese-CLIP/
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip

split=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾
resume=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt

python -u cn_clip/eval/extract_features.py \
    --extract-image-feats \
    --extract-text-feats \
    --image-data="${DATAPATH}/datasets/${dataset_name}/lmdb/${split}/imgs" \
    --text-data="${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl" \
    --img-batch-size=32 \
    --text-batch-size=32 \
    --context-length=52 \
    --resume=${resume} \
    --vision-model=ViT-B-16 \
    --text-model=RoBERTa-wwm-ext-base-chinese
```

äº§å‡ºå›¾æ–‡ç‰¹å¾é»˜è®¤å°†ä¿å­˜äº`${DATAPATH}/datasets/${dataset_name}`ç›®å½•ä¸‹ï¼Œå›¾ç‰‡ç‰¹å¾ä¿å­˜äº`${split}_imgs.img_feat.jsonl`æ–‡ä»¶ï¼Œæ¯è¡Œä»¥jsonå­˜å‚¨ä¸€å¼ å›¾ç‰‡çš„ç‰¹å¾ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```
{"image_id": 1000002, "feature": [0.0198, ..., -0.017, 0.0248]}
```
æ–‡æœ¬ç‰¹å¾åˆ™ä¿å­˜äº`${split}_texts.txt_feat.jsonl`ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```
{"text_id": 248816, "feature": [0.1314, ..., 0.0018, -0.0002]}
```

#### KNNæ£€ç´¢

å¯¹äºå°è§„æ¨¡çš„å­¦æœ¯æ£€ç´¢æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªç®€å•çš„KNNæ£€ç´¢å®ç°ï¼Œä¾¿äºè®¡ç®—æ–‡åˆ°å›¾ã€å›¾åˆ°æ–‡æ£€ç´¢çš„top-kå¬å›ç»“æœï¼ˆtipsï¼šå¦‚æƒ³ä»¿ç…§æˆ‘ä»¬åœ¨é¡¹ç›®ä¸­æ­å»ºæ£€ç´¢demoï¼Œå»ºè®®åŸºäºä¸­æ–‡CLIPæ¨¡å‹äº§å‡ºå›¾æ–‡ç‰¹å¾åï¼Œç»“åˆå¼€æºå·¥ç¨‹æ¡†æ¶[clip-retrieval](https://github.com/rom1504/clip-retrieval)æ­å»ºå‰åç«¯æœåŠ¡ã€‚ï¼‰

å¯¹äºæ–‡åˆ°å›¾æ£€ç´¢ï¼ˆæ–‡æœ¬å¬å›ç›¸å…³å›¾ç‰‡ï¼‰ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
cd Chinese-CLIP/
split=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾
python -u cn_clip/eval/make_topk_predictions.py \
    --image-feats="${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl" \
    --text-feats="${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl" \
    --top-k=10 \
    --eval-batch-size=32768 \
    --output="${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl"
```
äº§å‡ºçš„ç»“æœä¿å­˜åœ¨æŒ‡å®šçš„jsonlæ–‡ä»¶ä¸­ï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ–‡æœ¬å¬å›çš„top-kå›¾ç‰‡idï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{"text_id": 153915, "image_ids": [5791244, 1009692167, 7454547004, 3564007203, 38130571, 2525270674, 2195419145, 2503091968, 4966265765, 3690431163]}
```

å¯¹äºå›¾åˆ°æ–‡æ£€ç´¢ï¼ˆå›¾ç‰‡å¬å›ç›¸å…³æ–‡æœ¬ï¼‰ï¼Œç±»ä¼¼åœ°ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
split=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾
python -u cn_clip/eval/make_topk_predictions_tr.py \
    --image-feats="${DATAPATH}/datasets/${dataset_name}/${split}_imgs.img_feat.jsonl" \
    --text-feats="${DATAPATH}/datasets/${dataset_name}/${split}_texts.txt_feat.jsonl" \
    --top-k=10 \
    --eval-batch-size=32768 \
    --output="${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl"
```
äº§å‡ºç»“æœæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªå›¾ç‰‡å¬å›çš„top-kæ–‡æœ¬idï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{"image_id": 977856234, "text_ids": [156914, 157914, 158914, 155914, 156179, 158907, 157179, 154179, 154914, 154723]}
```

#### Recallè®¡ç®—

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬è®¡ç®—æ£€ç´¢ä»»åŠ¡çš„Recall@1/5/10ï¼ŒåŒæ—¶ç»™å‡ºmean recallï¼ˆRecall@1/5/10çš„å¹³å‡æ•°ï¼‰ã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è·å–åˆ†æ•°:

å¯¹äºæ–‡åˆ°å›¾æ£€ç´¢ï¼Œè¯·è¿è¡Œå‘½ä»¤ï¼š
```bash
split=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾
python cn_clip/eval/evaluation.py \
    ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl \
    ${DATAPATH}/datasets/${dataset_name}/${split}_predictions.jsonl \
    output.json
cat output.json
```

å¯¹äºå›¾åˆ°æ–‡æ£€ç´¢ï¼Œè¯·å…ˆè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œå°†å›¾æ–‡å¯¹æ ‡æ³¨çš„jsonlæ–‡ä»¶ç”±æ–‡åˆ°å›¾çš„æ ¼å¼è½¬ä¸ºå›¾åˆ°æ–‡ï¼š
```bash
python cn_clip/eval/transform_ir_annotation_to_tr.py \
    --input ${DATAPATH}/datasets/${dataset_name}/${split}_texts.jsonl
```
å®Œæˆåï¼Œè¯·è¿è¡Œå‘½ä»¤ï¼š
```bash
split=valid # æŒ‡å®šè®¡ç®—validæˆ–testé›†ç‰¹å¾
python cn_clip/eval/evaluation_tr.py \
    ${DATAPATH}/datasets/${dataset_name}/${split}_texts.tr.jsonl \
    ${DATAPATH}/datasets/${dataset_name}/${split}_tr_predictions.jsonl \
    output.json
cat output.json
```
æ‰“å°å‡ºçš„ç»“æœæ ¼å¼å°†å¦‚ä¸‹ï¼š
```json
{"success": true, "score": 85.67, "scoreJson": {"score": 85.67, "mean_recall": 85.67, "r1": 71.2, "r5": 90.5, "r10": 95.3}}
```
<br>


## é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
æœ¬éƒ¨åˆ†ä»‹ç»å¦‚ä½•ä½¿ç”¨Chinese CLIPå®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼Œä»¥é›¶æ ·æœ¬å›¾åƒåˆ†ç±»Benchmark ELEVATERä¸­çš„æ•°æ®é›†ä¸ºä¾‹ã€‚æ›´å¤šå…³äºè¯¥benchmarkçš„è¯¦æƒ…è¯·ç‚¹å‡»[é“¾æ¥](https://eval.ai/web/challenges/challenge-page/1832/overview)ã€‚
<br>

### å‡†å¤‡å·¥ä½œ
é¦–å…ˆå°†æ•°æ®æŒ‰ç…§å¦‚ä¸‹æ ¼å¼è¿›è¡Œå‡†å¤‡ã€‚ç”±äºé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ä»…éœ€æµ‹è¯•ï¼Œå› æ­¤åªéœ€è¦å‡†å¤‡å¥½æµ‹è¯•é›†ï¼š
```
${DATAPATH}
â””â”€â”€ datasets
    â””â”€â”€ ${dataset_name}
        â”œâ”€â”€ label_cn.txt
        â””â”€â”€ test
	    â”œâ”€â”€ 000 # label idï¼Œå¦‚labelä¸ªæ•°å¤§äº10ï¼Œåˆ™å°†å…¶å‘å·¦è¡¥é›¶åˆ°3ä½æ•°ä¿è¯å­—å…¸åº
	    â”‚   â”œâ”€â”€ image_0003.jpg # å›¾ç‰‡æ ·æœ¬ï¼Œå‘½åæ— ç‰¹æ®Šè¦æ±‚
	    â”‚   â”œâ”€â”€ image_0005.jpg
	    â”‚Â Â  â”œâ”€â”€ ...
	    â”œâ”€â”€ 001
	    â”‚Â Â  â”œâ”€â”€ image_0001.jpg
	    â”‚Â Â  â”œâ”€â”€ image_0002.jpg
	    â”‚Â Â  â”œâ”€â”€ ...
	    â””â”€â”€ 002
	        â”œâ”€â”€ image_0003.jpg
	        â”œâ”€â”€ image_0005.jpg
	        â””â”€â”€ ...
	    ...
	
```
æµ‹è¯•é›†ä¿è¯testæ–‡ä»¶å¤¹å†…æ•°æ®æŒ‰ç…§labelå¯¹åº”çš„idè¿›è¡Œåˆ’åˆ†ï¼Œå¹¶ä¿è¯idä¸ºå­—å…¸åºï¼ˆ10ä»¥ä¸Šçš„å¤šä½æ•°ï¼Œéœ€å‘å·¦è¡¥é›¶`label.zfill(3)`, å¦‚001ï¼Œ002ç­‰ï¼‰ã€‚`label_cn.txt`ä¸ºæ•°æ®æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ªæ ‡ç­¾åï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```
æ‰‹é£ç´
é£æœº
é”š
...
```
æ¯è¡Œçš„æ ‡ç­¾å¯¹åº”çš„label idä¸º`è¡Œå·-1`ï¼Œå¦‚ç¬¬1è¡Œçš„æ ‡ç­¾çš„idä¸º0ï¼Œç¬¬äºŒè¡Œçš„æ ‡ç­¾çš„idä¸º1ã€‚å¦‚æœæ ‡ç­¾æ€»æ•°å¤§äº10ï¼Œåˆ™ç»Ÿä¸€å‘å·¦è¡¥é›¶åˆ°3ä½æ•°ï¼Œæ¯”å¦‚æ ‡ç­¾ä¸ªæ•°ä¸º100ï¼Œæ ‡ç­¾idåˆ™ä¸º`000-099`ã€‚ç”¨æˆ·éœ€ä¸ºæ¯ä¸ªlabel idç”Ÿæˆå¯¹åº”çš„æ–‡ä»¶å¤¹ï¼Œå¹¶å°†æ ‡æ³¨è¯¥labelçš„æ ·æœ¬æ”¾å…¥å…¶ä¸­ã€‚æˆ‘ä»¬ä»¥FGVCä¸ºæ ·ä¾‹ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](https://shuangqing-multimodal.oss-cn-zhangjiakou.aliyuncs.com/cvinw/classification_organized/fgvc-aircraft-2013b-variants102-example.zip)ä¸‹è½½ã€‚
<br>

### é¢„æµ‹å’Œè¯„ä¼°
æˆ‘ä»¬å‡†å¤‡äº†é¢„æµ‹è„šæœ¬ï¼Œè¯·æŸ¥çœ‹`run_scripts/zeroshot_eval.sh`ã€‚è¿è¡Œå‘½ä»¤ä¾‹å­å¦‚ä¸‹ï¼š
```bash
bash run_scripts/zeroshot_eval.sh 0 \
    ${DATAPATH} ${dataset_name} \
    ${vision_model} ${text_model} \
    ${ckpt_path}
```
å…¶ä¸­ç¬¬ä¸€ä¸ªå…¥å‚`0`ä¸ºGPU idï¼Œ`vision_model`ä¸ºæŒ‡å®šæ¨¡å‹ç±»å‹ï¼Œé€‰é¡¹åŒ…æ‹¬`["ViT-B-32", "ViT-B-16", "ViT-L-14", "ViT-L-14-336", "RN50", "ViT-H-14"]`ï¼Œè€Œ`text_model`åŒ…æ‹¬`["RoBERTa-wwm-ext-base-chinese", "RoBERTa-wwm-ext-large-chinese", "RBT3-chinese"]`ï¼Œ`ckpt_path`å³ä¸ºæ¨¡å‹ckptçš„è·¯å¾„ã€‚

è¿”å›ç»“æœä¼šæ‰“å°top-1çš„å‡†ç¡®ç‡ã€‚åŒæ—¶ï¼Œç¨‹åºè¿˜ä¼šå­˜ä¸‹ä¸€ä¸ªjsonæ–‡ä»¶ç”¨äºæäº¤ELEVATERç”¨ï¼Œjsonæ–‡ä»¶å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š
```json
{"model_name": "CN-CLIP-ViT-B-16", "dataset_name": "fgvc-aircraft-2013b-variants102", "num_trainable_params": 0, "num_params": 188262913, "num_visual_params": 86192640, "num_backbone_params": 188262913 "n_shot": 0, "rnd_seeds": [0], "predictions": "prediction probability tensor [size: (1, 10000, 101)]"}
```
å…¶ä¸­åŒ…æ‹¬æ¨¡å‹å`model_name`ã€æ•°æ®é›†åç§°`dataset_name`ã€æ€»å‚æ•°é‡`num_params`ã€è§†è§‰å¡”çš„å‚æ•°é‡`num_visual_params`ç­‰æ¨¡å‹çš„metaä¿¡æ¯ï¼Œä»¥åŠæ¨¡å‹è¾“å‡ºç»“æœï¼Œå³æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡tensorï¼Œsizeä¸º`[1, æ ·æœ¬æ•°, æ ‡ç­¾ä¸ªæ•°]`ã€‚
<br><br><br>


# å¼•ç”¨
å¦‚æœè§‰å¾—æœ¬é¡¹ç›®å¥½ç”¨ï¼Œå¸Œæœ›èƒ½ç»™æˆ‘ä»¬æä¸ªstarå¹¶åˆ†äº«ç»™èº«è¾¹çš„ç”¨æˆ·ï¼Œæ¬¢è¿ç»™ç›¸å…³å·¥ä½œcitationï¼Œæ„Ÿè°¢æ”¯æŒï¼

```
@article{chinese-clip,
  title={Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese},
  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},
  journal={arXiv preprint arXiv:2211.01335},
  year={2022}
}
```
