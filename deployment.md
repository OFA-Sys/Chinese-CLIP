[**ä¸­æ–‡è¯´æ˜**](deployment.md) | [**English**](deployment_En.md)

# Chinese-CLIPæ¨¡å‹éƒ¨ç½²ï¼šONNX & TensorRTæ ¼å¼è½¬æ¢

æœ€æ–°çš„Chinese-CLIPä»£ç ï¼Œå·²æ”¯æŒå°†å„è§„æ¨¡çš„Pytorchæ¨¡å‹ï¼Œè½¬æ¢ä¸º[ONNX](https://onnx.ai/)æˆ–[TensorRT](https://developer.nvidia.com/tensorrt)æ ¼å¼ï¼Œä»è€Œç›¸æ¯”åŸå§‹Pytorchæ¨¡å‹ **[æå‡ç‰¹å¾è®¡ç®—çš„æ¨ç†é€Ÿåº¦](#é€Ÿåº¦å¯¹æ¯”ç»“æœ)**ï¼ŒåŒæ—¶ä¸å½±å“ç‰¹å¾æå–çš„ä¸‹æ¸¸ä»»åŠ¡æ•ˆæœã€‚ä¸‹é¢æˆ‘ä»¬ç»™å‡ºåœ¨GPUä¸Šï¼Œå‡†å¤‡ONNXå’ŒTensorRTæ ¼å¼çš„FP16 Chinese-CLIPéƒ¨ç½²æ¨¡å‹çš„æ•´ä¸ªæµç¨‹ï¼ˆåŒæ—¶ç»™å‡ºäº†Chinese-CLIPé¢„è®­ç»ƒTensorRTæ¨¡å‹çš„[ä¸‹è½½æ–¹å¼](#tensorrt_download)ï¼‰ï¼Œå¹¶é™„ä¸Šæ¨¡å‹æ•ˆæœå’Œæ¨ç†é€Ÿåº¦çš„å¯¹æ¯”ï¼Œæ–¹ä¾¿å¤§å®¶ä¸Šæ‰‹åˆ©ç”¨ONNXå’ŒTensorRTåº“åœ¨æ¨ç†æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ã€‚

## ç¯å¢ƒå‡†å¤‡

+ **GPUç¡¬ä»¶è¦æ±‚**ï¼šè¯·å‡†å¤‡**Voltaæ¶æ„åŠä»¥ä¸Š**çš„Nvidia GPUæ˜¾å¡ï¼ˆé…å¤‡FP16 Tensor Coreï¼‰ï¼ŒNvidiaå„æ¶æ„å¯¹åº”æ˜¾å¡å‹å·è¯·å‚è§[æ­¤æ–‡æ¡£è¡¨æ ¼](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)ã€‚æœ¬æ–‡æˆ‘ä»¬ä»¥T4æ˜¾å¡ä¸ºä¾‹
+ **CUDA**ï¼šæ¨è[CUDA](https://developer.nvidia.com/cuda-11-6-0-download-archive)ç‰ˆæœ¬11.6åŠä»¥ä¸Šï¼Œæœ¬æ–‡ä»¥11.6ä¸ºä¾‹
+ **CUDNN**ï¼šæ¨è[CUDNN](https://developer.nvidia.com/rdp/cudnn-archive) 8.6.0åŠä»¥ä¸Šï¼Œæœ¬æ–‡ä»¥8.6.0ä¸ºä¾‹ã€‚è¯·æ³¨æ„TensorRTå’ŒCUDNNæœ‰ç‰ˆæœ¬matchå…³ç³»ï¼Œå¦‚TensorRT 8.5.xå¿…é¡»ä½¿ç”¨CUDNN 8.6.0ï¼Œè¯¦è§TensorRTçš„ç‰ˆæœ¬è¦æ±‚
+ **ONNX**ï¼šæ³¨æ„æˆ‘ä»¬è½¬æ¢TensorRTæ¨¡å‹æ—¶ï¼Œå°†æ²¿ç€Pytorch â†’ ONNX â†’ TensorRTçš„æ­¥éª¤ï¼Œæ‰€ä»¥å‡†å¤‡TensorRTæ¨¡å‹ä¹Ÿéœ€è¦å…ˆå®‰è£…ONNXåº“ã€‚æœ¬æ–‡ä»¥onnxç‰ˆæœ¬1.13.0ï¼Œonnxruntime-gpuç‰ˆæœ¬1.13.1ï¼Œonnxmltoolsç‰ˆæœ¬1.11.1ä¸ºä¾‹
+ **TensorRT**ï¼šæ¨è[TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)ç‰ˆæœ¬8.5.xï¼Œæœ¬æ–‡ä»¥8.5.2.2ä¸ºä¾‹ã€‚TensorRTå„ç‰ˆæœ¬å¯¹åº”çš„CUDNNåŒ¹é…ç‰ˆæœ¬ï¼Œè¯·ä»[æ–‡æ¡£é¡µé¢](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8)ï¼ŒæŸ¥é˜…æ­¤TensorRTç‰ˆæœ¬çš„"NVIDIA TensorRT Support Matrix"
+ **Pytorch**ï¼šæ¨è1.12.1åŠä»¥ä¸Šï¼Œæœ¬æ–‡ä»¥1.12.1ä¸ºä¾‹ï¼ˆå»ºè®®ç›´æ¥pipå®‰è£…1.12.1+cu116ï¼Œç¯å¢ƒå°½é‡ä¸è¦å†ä½¿ç”¨condaå®‰è£…cudatoolkitï¼Œé¿å…ç¯å¢ƒCUDNNç‰ˆæœ¬å˜åŒ–ï¼Œå¯¼è‡´TensorRTæŠ¥é”™ï¼‰
+ [requirements.txt](requirements.txt)è¦æ±‚çš„å…¶ä»–ä¾èµ–é¡¹

æ‰§è¡Œä»£ç 
``` 
pip install tensorrt==8.5.2.2 onnx==1.13.0 onnxruntime-gpu==1.13.1 onnxmltools==1.11.1
pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt 
```

## è½¬æ¢å’Œè¿è¡ŒONNXæ¨¡å‹

### è½¬æ¢æ¨¡å‹

å°†Pytorchæ¨¡å‹checkpointè½¬æ¢ä¸ºONNXæ ¼å¼çš„ä»£ç ï¼Œè¯·å‚è§`cn_clip/deploy/pytorch_to_onnx.py`ã€‚æˆ‘ä»¬ä»¥è½¬æ¢ViT-B-16è§„æ¨¡çš„Chinese-CLIPé¢„è®­ç»ƒæ¨¡å‹ä¸ºä¾‹ï¼Œå…·ä½“çš„ä»£ç è¿è¡Œæ–¹å¼å¦‚ä¸‹ï¼ˆè¯·å‚è€ƒReadme[ä»£ç ç»„ç»‡éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#ä»£ç ç»„ç»‡)å»ºå¥½`${DATAPATH}`å¹¶æ›¿æ¢ä¸‹é¢çš„è„šæœ¬å†…å®¹ï¼Œå°½é‡ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰ï¼š

```bash
cd Chinese-CLIP/
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip

# ${DATAPATH}çš„æŒ‡å®šï¼Œè¯·å‚è€ƒReadme"ä»£ç ç»„ç»‡"éƒ¨åˆ†åˆ›å»ºå¥½ç›®å½•ï¼Œå°½é‡ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼šhttps://github.com/OFA-Sys/Chinese-CLIP#ä»£ç ç»„ç»‡
checkpoint_path=${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt # æŒ‡å®šè¦è½¬æ¢çš„ckptå®Œæ•´è·¯å¾„
mkdir -p ${DATAPATH}/deploy/ # åˆ›å»ºONNXæ¨¡å‹çš„è¾“å‡ºæ–‡ä»¶å¤¹

python cn_clip/deploy/pytorch_to_onnx.py \
       --model-arch ViT-B-16 \
       --pytorch-ckpt-path ${checkpoint_path} \
       --save-onnx-path ${DATAPATH}/deploy/vit-b-16 \
       --convert-text --convert-vision
```

å…¶ä¸­å„é…ç½®é¡¹å®šä¹‰å¦‚ä¸‹ï¼š
+ `model-arch`: æ¨¡å‹è§„æ¨¡ï¼Œé€‰é¡¹åŒ…æ‹¬`["RN50", "ViT-B-16", "ViT-L-14", "ViT-L-14-336", "ViT-H-14"]`ï¼Œå„è§„æ¨¡ç»†èŠ‚è¯¦è§[Readme](https://github.com/OFA-Sys/Chinese-CLIP#æ¨¡å‹è§„æ¨¡--ä¸‹è½½é“¾æ¥)
+ `pytorch-ckpt-path`: æŒ‡å®šPytorchæ¨¡å‹ckptè·¯å¾„ï¼Œä¸Šé¢çš„ä»£ç ç¤ºä¾‹ä¸­æˆ‘ä»¬æŒ‡å®šä¸ºé¢„è®­ç»ƒçš„ckptè·¯å¾„ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸ºç”¨æˆ·finetune ckptçš„ä½ç½®ã€‚ckptä¸­çš„å‚æ•°éœ€è¦ä¸`model-arch`æŒ‡å®šçš„æ¨¡å‹è§„æ¨¡å¯¹åº”
+ `save-onnx-path`: æŒ‡å®šè¾“å‡ºONNXæ ¼å¼æ¨¡å‹çš„è·¯å¾„ï¼ˆå‰ç¼€ï¼‰ã€‚å®Œæˆè½¬æ¢åï¼Œä»£ç å°†åˆ†åˆ«è¾“å‡ºæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„ONNXæ ¼å¼ç¼–ç æ¨¡å‹æ–‡ä»¶ï¼ŒFP32ä¸FP16å„ä¸€ç‰ˆï¼Œè¯¥å‚æ•°å³æŒ‡å®šäº†ä»¥ä¸Šè¾“å‡ºæ–‡ä»¶çš„è·¯å¾„å‰ç¼€
+ `convert-text`å’Œ`convert-vision`: æŒ‡å®šæ˜¯å¦è½¬æ¢æ–‡æœ¬ä¾§å’Œå›¾åƒä¾§æ¨¡å‹
+ `context-length`ï¼ˆå¯é€‰ï¼‰: æŒ‡å®šæ–‡æœ¬ä¾§ONNXæ¨¡å‹ï¼Œæ¥æ”¶è¾“å…¥çš„åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸ºæˆ‘ä»¬é¢„è®­ç»ƒckptæ‰€ä½¿ç”¨çš„52
+ `download-root`ï¼ˆå¯é€‰ï¼‰: å¦‚æœä¸æŒ‡å®š`pytorch-ckpt-path`ï¼Œä»£ç å°†æ ¹æ®`model-arch`è‡ªåŠ¨ä¸‹è½½Chinese-CLIPå®˜æ–¹é¢„è®­ç»ƒckptç”¨äºè½¬æ¢ï¼Œå­˜æ”¾äº`download-root`æŒ‡å®šçš„ç›®å½•

è¿è¡Œæ­¤ä»£ç è½¬æ¢å®Œæˆåï¼Œå°†å¾—åˆ°ä»¥ä¸‹çš„logè¾“å‡ºï¼š
```
Finished PyTorch to ONNX conversion...
>>> The text FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp32.onnx
>>> The text FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file
>>> The vision FP32 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp32.onnx
>>> The vision FP16 ONNX model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx with extra file ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx.extra_file
```

ä¸Šé¢ç¤ºä¾‹ä»£ç æ‰§è¡Œç»“æŸåï¼Œæˆ‘ä»¬å¾—åˆ°äº†ViT-B-16è§„æ¨¡ï¼ŒChinese-CLIPæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„ONNXæ¨¡å‹ï¼Œå¯ä»¥åˆ†åˆ«ç”¨äºæå–å›¾æ–‡ç‰¹å¾ã€‚è¾“å‡ºONNXæ¨¡å‹çš„è·¯å¾„å‡ä»¥è¿è¡Œè„šæœ¬æ—¶çš„`save-onnx-path`ä¸ºå‰ç¼€ï¼Œåé¢ä¾æ¬¡æ‹¼ä¸Š`.img`/`.txt`ã€`.fp16`/`.fp32`ã€`.onnx`ã€‚æˆ‘ä»¬åç»­å°†ä¸»è¦ä½¿ç”¨FP16æ ¼å¼çš„ONNXæ¨¡å‹`vit-b-16.txt.fp16.onnx`å’Œ`vit-b-16.img.fp16.onnx`

æ³¨æ„åˆ°éƒ¨åˆ†ONNXæ¨¡å‹æ–‡ä»¶è¿˜é™„å¸¦æœ‰ä¸€ä¸ªextra_fileï¼Œå…¶ä¹Ÿæ˜¯å¯¹åº”ONNXæ¨¡å‹çš„ä¸€éƒ¨åˆ†ã€‚åœ¨ä½¿ç”¨è¿™äº›ONNXæ¨¡å‹æ—¶ï¼Œç”±äº`.onnx`æ–‡ä»¶å­˜å‚¨äº†extra_fileçš„è·¯å¾„ï¼ˆå¦‚`${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx.extra_file`ï¼‰å¹¶ä¼šæ ¹æ®æ­¤è·¯å¾„è½½å…¥extra_fileï¼Œæ‰€ä»¥ä½¿ç”¨ONNXæ¨¡å‹è¯·ä¸è¦æ”¹åŠ¨å­˜æ”¾çš„è·¯å¾„åï¼Œè½¬æ¢æ—¶`${DATAPATH}`ä¹Ÿå°½é‡ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚`../datapath`ï¼‰ï¼Œé¿å…è¿è¡Œæ—¶æŒ‰è·¯å¾„æ‰¾ä¸åˆ°extra_fileæŠ¥é”™

### è¿è¡Œæ¨¡å‹

#### æå–å›¾åƒä¾§ç‰¹å¾
æˆ‘ä»¬åœ¨`Chinese-CLIP/`ç›®å½•ä¸‹ï¼Œä½¿ç”¨ä»¥ä¸‹çš„ç¤ºä¾‹ä»£ç ï¼Œè¯»å–åˆšåˆšè½¬æ¢å¥½çš„ViT-B-16è§„æ¨¡ONNXå›¾åƒä¾§æ¨¡å‹`vit-b-16.img.fp16.onnx`ï¼Œå¹¶ä¸ºReadmeä¸­ç¤ºä¾‹çš„[çš®å¡ä¸˜å›¾ç‰‡](examples/pokemon.jpeg)æå–å›¾åƒä¾§ç‰¹å¾ã€‚æ³¨æ„è½¬æ¢å¥½çš„ONNXæ¨¡å‹åªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€å¼ è¾“å…¥å›¾ç‰‡

```python
# å®Œæˆå¿…è¦çš„importï¼ˆä¸‹æ–‡çœç•¥ï¼‰
import onnxruntime
from PIL import Image
import numpy as np
import torch
import argparse
import cn_clip.clip as clip
from clip import load_from_name, available_models
from clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform

# è½½å…¥ONNXå›¾åƒä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰
img_sess_options = onnxruntime.SessionOptions()
img_run_options = onnxruntime.RunOptions()
img_run_options.log_severity_level = 2
img_onnx_model_path="${DATAPATH}/deploy/vit-b-16.img.fp16.onnx"
img_session = onnxruntime.InferenceSession(img_onnx_model_path,
                                        sess_options=img_sess_options,
                                        providers=["CUDAExecutionProvider"])

# é¢„å¤„ç†å›¾ç‰‡
model_arch = "ViT-B-16" # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ViT-B-16è§„æ¨¡ï¼Œå…¶ä»–è§„æ¨¡è¯·å¯¹åº”ä¿®æ”¹
preprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])
# ç¤ºä¾‹çš®å¡ä¸˜å›¾ç‰‡ï¼Œé¢„å¤„ç†åå¾—åˆ°[1, 3, åˆ†è¾¨ç‡, åˆ†è¾¨ç‡]å°ºå¯¸çš„Torch Tensor
image = preprocess(Image.open("examples/pokemon.jpeg")).unsqueeze(0)

# ç”¨ONNXæ¨¡å‹è®¡ç®—å›¾åƒä¾§ç‰¹å¾
image_features = img_session.run(["unnorm_image_features"], {"image": image.cpu().numpy()})[0] # æœªå½’ä¸€åŒ–çš„å›¾åƒç‰¹å¾
image_features = torch.tensor(image_features)
image_features /= image_features.norm(dim=-1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPå›¾åƒç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡
print(image_features.shape) # Torch Tensor shape: [1, ç‰¹å¾å‘é‡ç»´åº¦]
```

#### æå–æ–‡æœ¬ä¾§ç‰¹å¾

ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬ç”¨å¦‚ä¸‹ä»£ç å®Œæˆæ–‡æœ¬ä¾§ONNXæ¨¡å‹çš„è½½å…¥ä¸ç‰¹å¾è®¡ç®—ï¼Œä¸å›¾åƒä¾§ç›¸åŒï¼Œæ–‡æœ¬ä¾§ONNXéƒ¨ç½²æ¨¡å‹åªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€æ¡è¾“å…¥æ–‡æœ¬ã€‚æˆ‘ä»¬ä¸º4æ¡å€™é€‰æ–‡æœ¬ä¾æ¬¡è®¡ç®—ViT-B-16è§„æ¨¡æ¨¡å‹çš„æ–‡æœ¬ç‰¹å¾ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š

```python
# è½½å…¥ONNXæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰
txt_sess_options = onnxruntime.SessionOptions()
txt_run_options = onnxruntime.RunOptions()
txt_run_options.log_severity_level = 2
txt_onnx_model_path="${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx"
txt_session = onnxruntime.InferenceSession(txt_onnx_model_path,
                                        sess_options=txt_sess_options,
                                        providers=["CUDAExecutionProvider"])

# ä¸º4æ¡è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚åºåˆ—é•¿åº¦æŒ‡å®šä¸º52ï¼Œéœ€è¦å’Œè½¬æ¢ONNXæ¨¡å‹æ—¶ä¿æŒä¸€è‡´ï¼ˆå‚è§è½¬æ¢æ—¶çš„context-lengthå‚æ•°ï¼‰
text = clip.tokenize(["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"], context_length=52) 

# ç”¨ONNXæ¨¡å‹ä¾æ¬¡è®¡ç®—æ–‡æœ¬ä¾§ç‰¹å¾
text_features = []
for i in range(len(text)):
    one_text = np.expand_dims(text[i].cpu().numpy(),axis=0)
    text_feature = txt_session.run(["unnorm_text_features"], {"text":one_text})[0] # æœªå½’ä¸€åŒ–çš„æ–‡æœ¬ç‰¹å¾
    text_feature = torch.tensor(text_feature)
    text_features.append(text_feature)
text_features = torch.squeeze(torch.stack(text_features),dim=1) # 4ä¸ªç‰¹å¾å‘é‡stackåˆ°ä¸€èµ·
text_features = text_features / text_features.norm(dim=1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPæ–‡æœ¬ç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡
print(text_features.shape) # Torch Tensor shape: [4, ç‰¹å¾å‘é‡ç»´åº¦]
```

#### è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦

ONNXæ¨¡å‹äº§å‡ºçš„å½’ä¸€åŒ–å›¾æ–‡ç‰¹å¾ï¼Œå†…ç§¯åsoftmaxå³å¯è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆéœ€è¦è€ƒè™‘`logit_scale`ï¼‰ï¼Œä¸åŸå§‹Pytorchæ¨¡å‹æ“ä½œç›¸åŒï¼Œå‚è§ä»¥ä¸‹ä»£ç ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š

```python
# å†…ç§¯åsoftmax
# æ³¨æ„åœ¨å†…ç§¯è®¡ç®—æ—¶ï¼Œç”±äºå¯¹æ¯”å­¦ä¹ è®­ç»ƒæ—¶æœ‰temperatureçš„æ¦‚å¿µ
# éœ€è¦ä¹˜ä¸Šæ¨¡å‹logit_scale.exp()ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹logit_scaleå‡ä¸º4.6052ï¼Œæ‰€ä»¥è¿™é‡Œä¹˜ä»¥100
# å¯¹äºç”¨æˆ·è‡ªå·±çš„ckptï¼Œè¯·ä½¿ç”¨torch.loadè½½å…¥åï¼ŒæŸ¥çœ‹ckpt['state_dict']['module.logit_scale']æˆ–ckpt['state_dict']['logit_scale']
logits_per_image = 100 * image_features @ text_features.t()
print(logits_per_image.softmax(dim=-1)) # å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡: [[1.2252e-03, 5.2874e-02, 6.7116e-04, 9.4523e-01]]
```

å¯ä»¥çœ‹åˆ°ï¼Œç»™å‡ºçš„å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡ï¼Œå’Œ[Readmeä¸­å¿«é€Ÿä½¿ç”¨éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#apiå¿«é€Ÿä¸Šæ‰‹)ï¼ŒåŸºäºPytorchåŒä¸€ä¸ªæ¨¡å‹è®¡ç®—çš„ç»“æœåŸºæœ¬ä¸€è‡´ï¼Œè¯æ˜äº†ONNXæ¨¡å‹ç‰¹å¾è®¡ç®—çš„æ­£ç¡®æ€§ï¼Œè€Œ**ONNXæ¨¡å‹çš„ç‰¹å¾è®¡ç®—é€Ÿåº¦ç›¸æ¯”Pytorchæ›´æœ‰ä¼˜åŠ¿**ï¼ˆè¯¦è§[ä¸‹æ–‡](#é€Ÿåº¦å¯¹æ¯”ç»“æœ)ï¼‰ã€‚

## è½¬æ¢å’Œè¿è¡ŒTensorRTæ¨¡å‹

### è½¬æ¢æ¨¡å‹

ç›¸æ¯”ONNXæ¨¡å‹ï¼ŒTensorRTæ¨¡å‹å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæˆ‘ä»¬æä¾›äº†è½¬æ¢å¥½çš„Chinese-CLIPé¢„è®­ç»ƒTensorRTå›¾åƒä¾§å’Œæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆ[ä¸‹è½½æ–¹å¼](#tensorrt_download)ï¼‰ã€‚å¦‚å‰æ–‡æ‰€è¯´ï¼Œæˆ‘ä»¬å‡†å¤‡TensorRTæ ¼å¼æ¨¡å‹ï¼Œæ˜¯ç”¨åˆšåˆšå¾—åˆ°çš„ONNXæ ¼å¼æ¨¡å‹è¿›ä¸€æ­¥è½¬åŒ–è€Œæ¥ã€‚å°†ONNXè½¬æ¢ä¸ºTensorRTæ ¼å¼çš„ä»£ç ï¼Œè¯·å‚è§`cn_clip/deploy/onnx_to_tensorrt.py`ã€‚ä»ç„¶ä»¥ViT-B-16è§„æ¨¡ä¸ºä¾‹ï¼Œåˆ©ç”¨åˆšåˆšå¾—åˆ°çš„ONNXæ¨¡å‹`vit-b-16.txt.fp16.onnx`å’Œ`vit-b-16.img.fp16.onnx`ï¼Œåœ¨`Chinese-CLIP/`ä¸‹è¿è¡Œå¦‚ä¸‹ä»£ç ï¼š

```bash
export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip
# å¦‚å‰æ–‡ï¼Œ${DATAPATH}è¯·æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢
python cn_clip/deploy/onnx_to_tensorrt.py \
       --model-arch ViT-B-16 \
       --convert-text \
       --text-onnx-path ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \
       --convert-vision \
       --vision-onnx-path ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \
       --save-tensorrt-path ${DATAPATH}/deploy/vit-b-16 \
       --fp16
```

å…¶ä¸­å„é…ç½®é¡¹å®šä¹‰å¦‚ä¸‹ï¼š
+ `model-arch`: æ¨¡å‹è§„æ¨¡ï¼Œé€‰é¡¹åŒ…æ‹¬`["RN50", "ViT-B-16", "ViT-L-14", "ViT-L-14-336", "ViT-H-14"]`ï¼Œå„è§„æ¨¡ç»†èŠ‚è¯¦è§[Readme](https://github.com/OFA-Sys/Chinese-CLIP#æ¨¡å‹è§„æ¨¡--ä¸‹è½½é“¾æ¥)
+ `convert-text`å’Œ`convert-vision`: æŒ‡å®šæ˜¯å¦è½¬æ¢æ–‡æœ¬ä¾§å’Œå›¾åƒä¾§æ¨¡å‹
+ `text-onnx-path`: æŒ‡å®šæ–‡æœ¬ä¾§ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦ä¸`model-arch`æŒ‡å®šçš„æ¨¡å‹è§„æ¨¡å¯¹åº”
+ `vision-onnx-path`: æŒ‡å®šå›¾åƒä¾§ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œéœ€è¦ä¸`model-arch`æŒ‡å®šçš„æ¨¡å‹è§„æ¨¡å¯¹åº”
+ `save-tensorrt-path`: æŒ‡å®šè¾“å‡ºTensorRTæ ¼å¼æ¨¡å‹çš„è·¯å¾„ï¼ˆå‰ç¼€ï¼‰ã€‚å®Œæˆè½¬æ¢åï¼Œä»£ç å°†åˆ†åˆ«è¾“å‡ºæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„TensorRTæ ¼å¼ç¼–ç æ¨¡å‹æ–‡ä»¶ï¼Œè¯¥å‚æ•°å³æŒ‡å®šäº†ä»¥ä¸Šè¾“å‡ºæ–‡ä»¶çš„è·¯å¾„å‰ç¼€
+ `fp16`: æŒ‡å®šè½¬æ¢FP16ç²¾åº¦çš„TensorRTæ ¼å¼æ¨¡å‹

æ•´ä¸ªè¿‡ç¨‹æ ¹æ®æ¨¡å‹è§„æ¨¡ä¸åŒï¼Œè€—æ—¶å‡ åˆ†é’Ÿåˆ°åå‡ åˆ†é’Ÿä¸ç­‰ã€‚è¿è¡Œæ­¤ä»£ç è½¬æ¢å®Œæˆåï¼Œå°†å¾—åˆ°ä»¥ä¸‹çš„logè¾“å‡ºï¼š
```
Finished ONNX to TensorRT conversion...
>>> The text FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt
>>> The vision FP16 TensorRT model is saved at ${DATAPATH}/deploy/vit-b-16.img.fp16.trt
```

ä¸Šé¢ç¤ºä¾‹ä»£ç æ‰§è¡Œç»“æŸåï¼Œæˆ‘ä»¬ä½¿ç”¨ONNXæ¨¡å‹ï¼Œå¾—åˆ°äº†ViT-B-16è§„æ¨¡ï¼ŒChinese-CLIPæ–‡æœ¬ä¾§å’Œå›¾åƒä¾§çš„TensorRTæ ¼å¼æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºæå–å›¾æ–‡ç‰¹å¾ã€‚è¾“å‡ºTensorRTæ¨¡å‹çš„è·¯å¾„ä»¥è¿è¡Œè„šæœ¬æ—¶çš„`save-tensorrt-path`ä¸ºå‰ç¼€ï¼Œåé¢ä¾æ¬¡æ‹¼ä¸Š`.img`/`.txt`ã€`.fp16`ã€`.trt`ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªè¾“å‡ºæ–‡ä»¶`vit-b-16.txt.fp16.trt`å’Œ`vit-b-16.img.fp16.trt`ã€‚

**å¯¹äºå„è§„æ¨¡Chinese-CLIPé¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›è½¬æ¢å¥½çš„TensorRTå›¾åƒä¾§å’Œæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆåŸºäºTensorRT 8.5.2.2ç‰ˆæœ¬ï¼‰**ï¼Œä¸‹è½½æ–¹å¼å¦‚ä¸‹<span id="tensorrt_download"></span>ï¼š

<table border="1" width="120%">
    <tr align="center">
        <td><b>æ¨¡å‹è§„æ¨¡</b></td><td><b>TensorRTå›¾åƒä¾§æ¨¡å‹</b></td><td><b>TensorRTæ–‡æœ¬ä¾§æ¨¡å‹</b></td>
    </tr>
	<tr align="center">
        <td>CN-CLIP<sub>RN50</sub></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/rn50.img.fp16.trt">ğŸ¤—Download</a></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/rn50.txt.fp16.trt">ğŸ¤—Download</a></td>
    </tr>  
	<tr align="center">
        <td>CN-CLIP<sub>ViT-B/16</sub></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-b-16.img.fp16.trt">ğŸ¤—Download</a></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-b-16.txt.fp16.trt">ğŸ¤—Download</a></td>
    </tr>  
	<tr align="center">
        <td>CN-CLIP<sub>ViT-L/14</sub></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-l-14.img.fp16.trt">ğŸ¤—Download</a></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-l-14.txt.fp16.trt">ğŸ¤—Download</a></td>
    </tr>
	<tr align="center">
        <td>CN-CLIP<sub>ViT-L/14@336px</sub></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-l-14-336.img.fp16.trt">ğŸ¤—Download</a></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-l-14-336.txt.fp16.trt">ğŸ¤—Download</a></td>
    </tr>
	<tr align="center">
        <td>CN-CLIP<sub>ViT-H/14</sub></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-h-14.img.fp16.trt">ğŸ¤—Download</a></td><td><a href="https://huggingface.co/OFA-Sys/chinese-clip-trt/resolve/main/vit-h-14.txt.fp16.trt">ğŸ¤—Download</a></td>
    </tr>  
</table>
<br>

ä¸‹è½½åç›´æ¥ç½®äº`${DATAPATH}/deploy/`ä¸‹å³å¯

### è¿è¡Œæ¨¡å‹

åœ¨è¿è¡ŒTensorRTæ¨¡å‹æ—¶ï¼Œå¦‚æœè½¬æ¢å’Œè¿è¡Œä¸æ˜¯åœ¨åŒä¸€ä¸ªç¯å¢ƒä¸‹ï¼Œè¯·æ³¨æ„è¿è¡Œæ¨¡å‹çš„ç¯å¢ƒTensorRTåº“ç‰ˆæœ¬ä¸è½¬æ¢ä¿æŒä¸€è‡´ï¼Œé¿å…æŠ¥é”™

#### æå–å›¾åƒä¾§ç‰¹å¾

ç±»ä¼¼äºONNXæ¨¡å‹è¿è¡Œçš„æµç¨‹ï¼Œæˆ‘ä»¬åœ¨`Chinese-CLIP/`ç›®å½•ä¸‹ï¼Œä½¿ç”¨ä»¥ä¸‹çš„ç¤ºä¾‹ä»£ç ï¼Œè¯»å–åˆšåˆšè½¬æ¢å¥½çš„ViT-B-16è§„æ¨¡TensorRTå›¾åƒä¾§æ¨¡å‹`vit-b-16.img.fp16.trt`ï¼Œå¹¶ä¸ºReadmeä¸­ç¤ºä¾‹çš„[çš®å¡ä¸˜å›¾ç‰‡](examples/pokemon.jpeg)æå–å›¾åƒä¾§ç‰¹å¾ã€‚å’ŒONNXæ¨¡å‹ä¸€æ ·ï¼Œè¿™é‡Œè½¬æ¢å¥½çš„TensorRTæ¨¡å‹ä¹Ÿåªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€å¼ è¾“å…¥å›¾ç‰‡

```python
# å®Œæˆå¿…è¦çš„importï¼ˆä¸‹æ–‡çœç•¥ï¼‰
from cn_clip.deploy.tensorrt_utils import TensorRTModel
from PIL import Image
import numpy as np
import torch
import argparse
import cn_clip.clip as clip
from clip import load_from_name, available_models
from clip.utils import _MODELS, _MODEL_INFO, _download, available_models, create_model, image_transform

# è½½å…¥TensorRTå›¾åƒä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰
img_trt_model_path="${DATAPATH}/deploy/vit-b-16.img.fp16.trt"
img_trt_model = TensorRTModel(img_trt_model_path)

# é¢„å¤„ç†å›¾ç‰‡
model_arch = "ViT-B-16" # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ViT-B-16è§„æ¨¡ï¼Œå…¶ä»–è§„æ¨¡è¯·å¯¹åº”ä¿®æ”¹
preprocess = image_transform(_MODEL_INFO[model_arch]['input_resolution'])
# ç¤ºä¾‹çš®å¡ä¸˜å›¾ç‰‡ï¼Œé¢„å¤„ç†åå¾—åˆ°[1, 3, åˆ†è¾¨ç‡, åˆ†è¾¨ç‡]å°ºå¯¸çš„Torch Tensor
image = preprocess(Image.open("examples/pokemon.jpeg")).unsqueeze(0).cuda()

# ç”¨TensorRTæ¨¡å‹è®¡ç®—å›¾åƒä¾§ç‰¹å¾
image_features = img_trt_model(inputs={'image': image})['unnorm_image_features'] # æœªå½’ä¸€åŒ–çš„å›¾åƒç‰¹å¾
image_features /= image_features.norm(dim=-1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPå›¾åƒç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡
print(image_features.shape) # Torch Tensor shape: [1, ç‰¹å¾å‘é‡ç»´åº¦]
```

#### æå–æ–‡æœ¬ä¾§ç‰¹å¾

ä¸å›¾åƒä¾§ç±»ä¼¼ï¼Œæˆ‘ä»¬ç”¨å¦‚ä¸‹ä»£ç å®Œæˆæ–‡æœ¬ä¾§TensorRTæ¨¡å‹çš„è½½å…¥ä¸ç‰¹å¾è®¡ç®—ï¼Œä¸å›¾åƒä¾§ç›¸åŒï¼Œæ–‡æœ¬ä¾§TensorRTéƒ¨ç½²æ¨¡å‹åªæ¥å—batchå¤§å°ä¸º1çš„è¾“å…¥ï¼Œå³ä¸€æ¬¡è°ƒç”¨åªå¤„ç†ä¸€æ¡è¾“å…¥æ–‡æœ¬ã€‚TensorRTæ¥å—çš„æ–‡æœ¬åºåˆ—é•¿åº¦å’Œç”¨äºè½¬æ¢çš„ONNXæ¨¡å‹ä¸€è‡´ï¼Œè¯·å‚è§ONNXè½¬æ¢æ—¶çš„context-lengthå‚æ•°ã€‚æˆ‘ä»¬ä¸º4æ¡å€™é€‰æ–‡æœ¬ä¾æ¬¡è®¡ç®—ViT-B-16è§„æ¨¡æ¨¡å‹çš„æ–‡æœ¬ç‰¹å¾ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š

```python
# è½½å…¥TensorRTæ–‡æœ¬ä¾§æ¨¡å‹ï¼ˆ**è¯·æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„**ï¼‰
txt_trt_model_path="${DATAPATH}/deploy/vit-b-16.txt.fp16.trt"
txt_trt_model = TensorRTModel(txt_trt_model_path)

# ä¸º4æ¡è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚åºåˆ—é•¿åº¦æŒ‡å®šä¸º52ï¼Œéœ€è¦å’Œè½¬æ¢ONNXæ¨¡å‹æ—¶ä¿æŒä¸€è‡´ï¼ˆå‚è§ONNXè½¬æ¢æ—¶çš„context-lengthå‚æ•°ï¼‰
text = clip.tokenize(["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"], context_length=52).cuda()

# ç”¨TensorRTæ¨¡å‹ä¾æ¬¡è®¡ç®—æ–‡æœ¬ä¾§ç‰¹å¾
text_features = []
for i in range(len(text)):
    # æœªå½’ä¸€åŒ–çš„æ–‡æœ¬ç‰¹å¾
    text_feature = txt_trt_model(inputs={'text': torch.unsqueeze(text[i], dim=0)})['unnorm_text_features']
    text_features.append(text_feature)
text_features = torch.squeeze(torch.stack(text_features), dim=1) # 4ä¸ªç‰¹å¾å‘é‡stackåˆ°ä¸€èµ·
text_features = text_features / text_features.norm(dim=1, keepdim=True) # å½’ä¸€åŒ–åçš„Chinese-CLIPæ–‡æœ¬ç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡
print(text_features.shape) # Torch Tensor shape: [4, ç‰¹å¾å‘é‡ç»´åº¦]
```

#### è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦

TensorRTæ¨¡å‹äº§å‡ºçš„å½’ä¸€åŒ–å›¾æ–‡ç‰¹å¾ï¼Œå†…ç§¯åsoftmaxå³å¯è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆéœ€è¦è€ƒè™‘`logit_scale`ï¼‰ï¼Œä¸åŸå§‹Pytorchå’ŒONNXæ¨¡å‹çš„æ“ä½œå‡ç›¸åŒï¼Œå‚è§ä»¥ä¸‹ä»£ç ã€‚importç›¸å…³ä»£ç ä¸ä¸Šæ–‡ç›¸åŒï¼Œè¿™é‡Œçœç•¥ï¼š
```python
# å†…ç§¯åsoftmax
# æ³¨æ„åœ¨å†…ç§¯è®¡ç®—æ—¶ï¼Œç”±äºå¯¹æ¯”å­¦ä¹ è®­ç»ƒæ—¶æœ‰temperatureçš„æ¦‚å¿µ
# éœ€è¦ä¹˜ä¸Šæ¨¡å‹logit_scale.exp()ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹logit_scaleå‡ä¸º4.6052ï¼Œæ‰€ä»¥è¿™é‡Œä¹˜ä»¥100
# å¯¹äºç”¨æˆ·è‡ªå·±çš„ckptï¼Œè¯·ä½¿ç”¨torch.loadè½½å…¥åï¼ŒæŸ¥çœ‹ckpt['state_dict']['module.logit_scale']æˆ–ckpt['state_dict']['logit_scale']
logits_per_image = 100 * image_features @ text_features.t()
print(logits_per_image.softmax(dim=-1)) # å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡: [[1.2475e-03, 5.3037e-02, 6.7583e-04, 9.4504e-01]]
```

å¯ä»¥çœ‹åˆ°ï¼ŒTensorRTæ¨¡å‹ç»™å‡ºçš„å›¾æ–‡ç›¸ä¼¼æ¦‚ç‡ï¼Œå’Œ[Readmeä¸­å¿«é€Ÿä½¿ç”¨éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#apiå¿«é€Ÿä¸Šæ‰‹)åŸºäºPytorchçš„åŒä¸€ä»½æ¨¡å‹ã€ä»¥åŠä¸Šæ–‡ONNXæ¨¡å‹è®¡ç®—çš„ç»“æœéƒ½åŸºæœ¬ä¸€è‡´ï¼Œè¯æ˜äº†TensorRTæ¨¡å‹ç‰¹å¾è®¡ç®—çš„æ­£ç¡®æ€§ï¼Œè€ŒTensorRTæ¨¡å‹çš„ç‰¹å¾è®¡ç®—é€Ÿåº¦ï¼Œ**ç›¸æ¯”å‰ä¸¤è€…éƒ½æ›´èƒœä¸€ç­¹**ï¼ˆè¯¦è§[ä¸‹æ–‡](#é€Ÿåº¦å¯¹æ¯”ç»“æœ)ï¼‰ã€‚

## æ¨ç†é€Ÿåº¦å¯¹æ¯”

### å¯¹æ¯”å®éªŒè®¾ç½®

æˆ‘ä»¬çš„é€Ÿåº¦å¯¹æ¯”å®éªŒï¼Œåœ¨ä¸€å°å•å¡T4 GPUï¼ˆ16GBæ˜¾å­˜ï¼‰æœºå™¨è¿›è¡Œï¼Œé…å¤‡16ä¸ªIntel Xeon(Skylake) Platinum 8163 (2.5GHz) CPU coresï¼Œ64GBå†…å­˜ã€‚è¿›è¡Œé€Ÿåº¦æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸Šé¢çš„ç¤ºä¾‹å›¾ç‰‡å’Œå…¶ä¸­ä¸€ä¸ªå€™é€‰æ–‡æœ¬ï¼Œå¯¹Pytorchã€ONNXå’ŒTensorRTæ¨¡å‹å‡æ‰§è¡Œ100æ¬¡å›¾æ–‡ç‰¹å¾æå–ï¼Œå–è€—æ—¶å¹³å‡å€¼(ms)ã€‚ä»¥ViT-B-16è§„æ¨¡æµ‹é€Ÿä¸ºä¾‹ï¼Œåœ¨`Chinese-CLIP/`ä¸‹æ‰§è¡Œçš„ä»£ç å¦‚ä¸‹ï¼š
```bash
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip

# æ›¿æ¢${DATAPATH}ä¸ºå®é™…çš„è·¯å¾„
python3 cn_clip/deploy/speed_benchmark.py \
        --model-arch ViT-B-16 \
        --pytorch-ckpt ${DATAPATH}/pretrained_weights/clip_cn_vit-b-16.pt \
        --pytorch-precision fp16 \
        --onnx-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.onnx \
        --onnx-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.onnx \
        --tensorrt-image-model ${DATAPATH}/deploy/vit-b-16.img.fp16.trt \
        --tensorrt-text-model ${DATAPATH}/deploy/vit-b-16.txt.fp16.trt
```

åœ¨logè¾“å‡ºä¸­å°†å…ˆåæ‰“å°ä»¥ä¸‹å‡ è¡Œï¼Œå³ä¸ºæµ‹é€Ÿç»“æœï¼š
```
[Pytorch image inference speed (batch-size: 1):] mean=11.12ms, sd=0.05ms, min=11.00ms, max=11.32ms, median=11.11ms, 95p=11.20ms, 99p=11.30ms
[ONNX image inference speed (batch-size: 1):] mean=4.92ms, sd=0.04ms, min=4.82ms, max=5.01ms, median=4.92ms, 95p=4.98ms, 99p=5.00ms
[TensorRT image inference speed (batch-size: 1):] mean=3.58ms, sd=0.08ms, min=3.30ms, max=3.72ms, median=3.58ms, 95p=3.70ms, 99p=3.72ms

[Pytorch text inference speed (batch-size: 1):] mean=12.47ms, sd=0.07ms, min=12.32ms, max=12.64ms, median=12.48ms, 95p=12.57ms, 99p=12.61ms
[ONNX text inference speed (batch-size: 1):] mean=3.42ms, sd=0.44ms, min=2.96ms, max=3.89ms, median=3.45ms, 95p=3.87ms, 99p=3.88ms
[TensorRT text inference speed (batch-size: 1):] mean=1.54ms, sd=0.01ms, min=1.51ms, max=1.57ms, median=1.54ms, 95p=1.56ms, 99p=1.56ms
```

### é€Ÿåº¦å¯¹æ¯”ç»“æœ

æˆ‘ä»¬åˆ—å‡ºæ¨ç†batch sizeä¸º1çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè§„æ¨¡Pytorchã€ONNXå’ŒTensorRTæ¨¡å‹çš„FP16ç²¾åº¦æ¨ç†è€—æ—¶å¯¹æ¯”ï¼Œå¯ä»¥çœ‹åˆ°TensorRTå¯¹äºå°è§„æ¨¡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦æå‡å°¤å…¶æ˜æ˜¾
<table border="1" width="120%">
    <tr align="center">
        <th>å•ä½: ms/æ ·æœ¬</th><th colspan="3">å›¾åƒç‰¹å¾æå–</th><th colspan="3">æ–‡æœ¬ç‰¹å¾æå–</th>
    </tr>
    <tr align="center">
        <td>æ¨¡å‹</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td><td>Pytorch</td><td>ONNX</td><td>TensorRT</td>
    </tr>
	<tr align="center">
        <td width="120%">CN-CLIP<sub>RN50</sub></td><td>12.93</td><td>5.04</td><td><b>1.36</b></td><td>3.64</td><td>0.95</td><td><b>0.58</b></td>
    </tr>  
	<tr align="center">
        <td width="120%">CN-CLIP<sub>ViT-B/16</sub></td><td>11.12</td><td>4.92</td><td><b>3.58</b></td><td>12.47</td><td>3.42</td><td><b>1.54</b></td>
    </tr>  
	<tr align="center">
        <td width="120%">CN-CLIP<sub>ViT-L/14</sub></td><td>21.19</td><td>17.10</td><td><b>13.08</b></td><td>12.45</td><td>3.48</td><td><b>1.52</b></td>
    </tr>
	<tr align="center">
        <td width="120%">CN-CLIP<sub>ViT-L/14@336px</sub></td><td>47.11</td><td>48.40</td><td><b>31.59</b></td><td>12.24</td><td>3.25</td><td><b>1.54</b></td>
    </tr>
	<tr align="center">
        <td width="120%">CN-CLIP<sub>ViT-H/14</sub></td><td>35.10</td><td>34.00</td><td><b>26.98</b></td><td>23.98</td><td>6.01</td><td><b>3.89</b></td>
    </tr>  
</table>
<br>

## ä¸‹æ¸¸æ•ˆæœå¯¹æ¯”

æˆ‘ä»¬ä½¿ç”¨Chinese-CLIPå®éªŒä¸­ï¼Œæ‰€æ¶‰åŠçš„MUGEå›¾æ–‡æ£€ç´¢ä»»åŠ¡å¯¹æ¯”ä¸‹æ¸¸æ•ˆæœï¼Œè§‚å¯ŸPytorchã€ONNXå’ŒTensorRT FP16æ¨¡å‹zero-shotçš„è¡¨ç°ã€‚å¦‚[Readmeé¢„æµ‹åŠè¯„ä¼°éƒ¨åˆ†](https://github.com/OFA-Sys/Chinese-CLIP#é¢„æµ‹åŠè¯„ä¼°)éƒ¨åˆ†æ‰€è¿°ï¼ŒMUGEå›¾æ–‡æ£€ç´¢è¯„æµ‹ç»“æœåˆ†ä¸ºå›¾æ–‡ç‰¹å¾æå–ã€KNNæ£€ç´¢å’ŒRecallè®¡ç®—3æ­¥ã€‚ONNXå’ŒTensorRTæ¨¡å‹çš„å›¾æ–‡ç‰¹å¾æå–è„šæœ¬ï¼Œè¯·åˆ†åˆ«å‚è§`cn_clip/eval/extract_features_onnx.py`å’Œ`cn_clip/eval/extract_features_tensorrt.py`ï¼Œç›¸æ¯”äºåŸç”ŸPytorchç‰¹å¾æå–ä½¿ç”¨çš„`extract_features.py`ä»…åšäº†å¾®å°çš„æ”¹åŠ¨ã€‚åç»­çš„KNNå’ŒRecallè®¡ç®—ä½¿ç”¨çš„è„šæœ¬å’Œæµç¨‹å®Œå…¨ä¸å˜ã€‚

æˆ‘ä»¬é€‰å–ViT-B-16å’ŒViT-H-14ä¸¤ä¸ªè§„æ¨¡ï¼Œç»“æœå¯¹æ¯”å¦‚ä¸‹ï¼š
<table border="1" width="100%">
    <tr align="center">
        <th>Setup</th><th colspan="4">ViT-B-16 Zero-shot</th><th colspan="4">ViT-H-14 Zero-shot</th>
    </tr>
    <tr align="center">
        <td>Metric</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td>
    </tr>
	<tr align="center">
        <td width="120%">Pytorch FP16</sub></td><td>52.1</td><td>76.7</td><td>84.4</td><td>71.1</td><td>63.0</td><td>84.1</td><td>89.2</td><td>78.8</td>
    </tr>  
	<tr align="center">
        <td width="120%">ONNX FP16</sub></td><td>52.0</td><td>76.8</td><td>84.3</td><td>71.1</td><td>63.1</td><td>84.1</td><td>89.0</td><td>78.8</td>
    </tr>
	<tr align="center">
        <td width="120%">TensorRT FP16</sub></td><td>52.0</td><td>76.8</td><td>84.2</td><td>71.0</td><td>63.1</td><td>84.2</td><td>89.1</td><td>78.8</td>
    </tr>
</table>
<br>
ç»“æœæŒ‡æ ‡åŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œç›¸å·®Â±0.2åœ¨å¯ä»¥æ¥å—çš„èŒƒå›´å†…ï¼ˆæ¢ä¸€å°æœºå™¨å³å¯èƒ½é€ æˆçš„è¯¯å·®é‡çº§ï¼‰ï¼Œè¯æ˜äº†ONNXå’ŒTensorRTæ¨¡å‹çš„è½¬æ¢æ­£ç¡®æ€§ã€‚
